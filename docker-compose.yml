version: '3.8'

services:
  # Llama-cpp HTTP server for testing
  llama-server:
    build:
      context: .
      dockerfile: docker/Dockerfile.llama
    container_name: llama-server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL_PATH=/app/models/llama-2-7b-chat-q2k.gguf
      - HOST=0.0.0.0
      - PORT=8080
      - N_CTX=2048
      - N_THREADS=4
    working_dir: /app
    command: python -m llama_cpp.server --model /app/models/llama-2-7b-chat-q2k.gguf --host 0.0.0.0 --port 8080 --n_ctx 2048 --n_threads 4 --n_gpu_layers 0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5

  enhanced-gptcache:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: enhanced-gptcache
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./config.yaml:/app/config.yaml
      - ./src:/app/src
      - ./benchmark:/app/benchmark
      - ./tests:/app/tests
    environment:
      - PYTHONPATH=/app
      - CACHE_SIZE_MB=100
      - CONTEXT_ENABLED=true
      - PCA_ENABLED=true
      - FEDERATED_ENABLED=true
      - LLAMA_SERVER_URL=http://llama-server:8080
    working_dir: /app
    depends_on:
      llama-server:
        condition: service_healthy
    command: --help

  # Development service with live code reloading
  dev:
    build: 
      context: .
      dockerfile: docker/Dockerfile
    container_name: enhanced-gptcache-dev
    volumes:
      - .:/app
    environment:
      - PYTHONPATH=/app
      - DEVELOPMENT=true
    working_dir: /app
    command: shell
    stdin_open: true
    tty: true

  # Test service
  test:
    build: 
      context: .
      dockerfile: docker/Dockerfile
    container_name: enhanced-gptcache-test
    volumes:
      - ./src:/app/src
      - ./tests:/app/tests
      - ./config.yaml:/app/config.yaml
    environment:
      - PYTHONPATH=/app
      - TESTING=true
    working_dir: /app
    command: test

  # Benchmark service
  benchmark:
    build: 
      context: .
      dockerfile: docker/Dockerfile
    container_name: enhanced-gptcache-benchmark
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./src:/app/src
      - ./benchmark:/app/benchmark
      - ./config.yaml:/app/config.yaml
    environment:
      - PYTHONPATH=/app
      - BENCHMARK_OUTPUT_DIR=/app/data/results
    working_dir: /app
    command: generate-queries --output data/queries.json --count 100
